"""
IMPORTANT NOTE:

The code for this section is meant to run in a Google Colab notebook.

Visit this url in your web browser to view, edit and run the code:

https://tinyurl.com/generatenews

A copy of the code is also provided below for your reference,
but it won't run out-of-the-box since it is meant to run in the
Google Colab environment.

---

Project: Generating Realistic News Articles

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gJ7KHclSbhf1UTH31ucJeDkH9bqovdKn

### Step 1: Configuration
"""

DOMAIN_STYLE_TO_EMULATE = "www.nytimes.com"
RSS_FEED_URL = "https://rss.nytimes.com/services/xml/rss/nyt/US.xml"

"""### Step 2: Download Grover code and install requirements"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content
# !git clone https://github.com/rowanz/grover.git
# %cd /content/grover
# !python3 -m pip install regex jsonlines twitter-text-python feedparser

"""### Step 3: Download Grover Pre-Trained 'Mega' Model"""

import os
import requests

model_type = "mega"

model_dir = os.path.join('/content/grover/models', model_type)
if not os.path.exists(model_dir):
    os.makedirs(model_dir)

for ext in ['data-00000-of-00001', 'index', 'meta']:
    r = requests.get(f'https://storage.googleapis.com/grover-models/{model_type}/model.ckpt.{ext}', stream=True)
    with open(os.path.join(model_dir, f'model.ckpt.{ext}'), 'wb') as f:
        file_size = int(r.headers["content-length"])
        if file_size < 1000:
            raise ValueError("File doesn't exist? idk")
        chunk_size = 1000
        for chunk in r.iter_content(chunk_size=chunk_size):
            f.write(chunk)
    print(f"Just downloaded {model_type}/model.ckpt.{ext}!", flush=True)

"""### Step 4: Generate Fake Blog Entries and Post to Wordpress"""

# Commented out IPython magic to ensure Python compatibility.
# % tensorflow_version 1.x
import tensorflow as tf
import numpy as np
import sys
import feedparser
import time
from textwrap import fill
from ttp import ttp

sys.path.append('../')
from lm.modeling import GroverConfig, sample
from sample.encoder import get_encoder, _tokenize_article_pieces, extract_generated_target
import random


def get_articles_from_real_blog(domain, feed_url):
    """
    Given an RSS feed url, grab all the stories and format them as article objects
    suitable for feeding into Grover to generate replica stories.
    """
    feed_data = feedparser.parse(feed_url)
    articles = []
    for post in feed_data.entries:
        if 'published_parsed' in post:
            publish_date = time.strftime('%m-%d-%Y', post.published_parsed)
        else:
            publish_date = time.strftime('%m-%d-%Y')

        articles.append({
            'title': post.title,
            'text': '',
            'summary': '',
            'authors': ["Staff Writer"],
            'publish_date': publish_date,
            'domain': domain,
        })

    return articles



def generate_article_attribute(session, encoder, tokens, probs, article, target='article'):

    """
    Given attributes about an article (title, author, etc), use that context to generate
    a replacement for one of those attributes using the Grover model.

    This function is based on the Grover examples distributed with the Grover code.
    """

    # Tokenize the raw article text
    article_pieces = _tokenize_article_pieces(encoder, article)

    # Grab the article elements the model careas about - domain, date, title, etc.
    context_formatted = []
    for key in ['domain', 'date', 'authors', 'title', 'article']:
        if key != target:
            context_formatted.extend(article_pieces.pop(key, []))

    # Start formatting the tokens in the way the model expects them, starting with
    # which article attribute we want to generate.
    context_formatted.append(encoder.__dict__[f"begin_{target}"])
    # Tell the model which special tokens shouldn't be generated.
    ignore_ids_np = np.array(encoder.special_tokens_onehot)
    # As an exception, allow it to generate the end token.
    ignore_ids_np[encoder.__dict__[f"end_{target}"]] = 0

    # Grover predicts which words are the most likely next word. But
    # instead of always picking the single most likely next word, 
    # it uses a technique called nucleus sampling to pick likely words
    # while retaining some variety. This paremeter controls how much
    # variety there is in the word choice.
    article['top_ps'] = [0.95]

    # Run the input through the TensorFlow model and grab the generated output
    tokens_out, probs_out = session.run(
        [tokens, probs],
        feed_dict={
            # Pass real values for the inputs that the
            # model needs to be able to run.
            initial_context: [context_formatted],
            eos_token: encoder.__dict__[f"end_{target}"],
            ignore_ids: ignore_ids_np,
            p_for_topp: np.array([0.95]),
        }
    )

    # The model is done! Grab the results it generated.
    extraction = extract_generated_target(
        output_tokens=tokens_out[0], 
        encoder=encoder, 
        target=target
    )
    return extraction['extraction']


# Ready to start grabbing RSS feeds
domain = DOMAIN_STYLE_TO_EMULATE
articles = get_articles_from_real_blog(domain, RSS_FEED_URL)

# Instead of using an RSS feed as the source of headlines,
# you can uncomment this to make up your own headlines:
# articles = [
#     {
#         'title': "Government confirms that Machine Learning is Fun!",
#         'text': '',
#         'summary': '',
#         'authors': ["Staff Writer"],
#         'publish_date': '10-31-2019',
#         'domain': 'nytimes.com',                    
#     }
# ]

# Load the pre-trained "huge" Grover model with 1.5 billion params
model_config_fn = '/content/grover/lm/configs/mega.json'
model_ckpt = '/content/grover/models/mega/model.ckpt'
encoder = get_encoder()
news_config = GroverConfig.from_json_file(model_config_fn)

# Set up TensorFlow session to make predictions
tf_config = tf.ConfigProto(allow_soft_placement=True)

with tf.Session(config=tf_config, graph=tf.Graph()) as session:
    # Create the placehodler TensorFlow input variables needed to feed data to Grover model
    # to make new predictions.
    initial_context = tf.placeholder(tf.int32, [1, None])
    p_for_topp = tf.placeholder(tf.float32, [1])
    eos_token = tf.placeholder(tf.int32, [])
    ignore_ids = tf.placeholder(tf.bool, [news_config.vocab_size])

    # Initialize the array of tokens that the pre-trained model knows.
    tokens, probs = sample(
        news_config=news_config,
        initial_context=initial_context,
        eos_token=eos_token,
        ignore_ids=ignore_ids,
        p_for_topp=p_for_topp,
        do_topk=False
    )

    # Restore the pre-trained Grover 'huge' model weights
    saver = tf.train.Saver()
    saver.restore(session, model_ckpt)

    # Loop through each headline we scraped from an RSS feed or made up
    for article in articles:
        print(f"Generating new article based on headline '{article['title']}'")
        
        # Generate a new article body that fits the real headline
        article['text'] = generate_article_attribute(session, encoder, tokens, probs, article, target="article")

        # Now, generate a fake headline that better fits the generated article body
        # This replaces the real headline so none of the original article content remains
        article['title'] = generate_article_attribute(session, encoder, tokens, probs, article, target="title")

        # Grab generated text results so we can post them to WordPress
        article_title = article['title']
        article_text = article['text']

        print(f"Generated new article!")
        print(f"Headline: {article_title}")
        print(f"Body: {fill(article_text)}")
        print()
